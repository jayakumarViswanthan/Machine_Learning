# -*- coding: utf-8 -*-
"""Diabetes_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CuXmF7Vx7OM1_JYhnQRG05gv0zkjzdUF

**The objective of the dataset is to predict whether a patient has diabetes or not.**

**Importing the Dependencies**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

#loading the diabetes dataset to a pandas DataFrame
df=pd.read_csv("diabetes.csv")

"""
**Pregnancies** : To express the Number of pregnancies

**Glucose**: To express the Glucose level in blood

**BloodPressure**: To express the Blood pressure measurement

**SkinThickness**: To express the thickness of the skin

**Insulin**: To express the Insulin level in blood

**BMI**: To express the Body mass index

**DiabetesPedigreeFunction**: To express the Diabetes percentage

**Age**: To express the age

**Outcome**: To express the final result 1 is **Yes** and 0 is **No**"""

# printing the first 5 rows of the dataset
df.head()

"""## **Exploratory Data Analysis**"""

#checking for number of nulls present in each column
df.isnull().sum()

# number of rows and Columns in this dataset
df.shape

#getting info for the dataset
df.info()

# getting the statistical measures of the data
df.describe()

"""**Observation**

From the above statistical measures we can find that some of the value for the columns(Pregnancies,Glucose,BloodPressure,SkinThickness,Insulin,BMI) repesent zero in min. But these cannot be zero.So we assign those value with the mean of thier corresponding column.

# **Data Cleaning**
"""

# checking for value 0 in every column, since we are considering 0 as null in the dataset.
print("Pregnancies",df[df["Pregnancies"]==0].shape)
print("Glucose",df[df["Glucose"]==0].shape)
print("BloodPressure",df[df["BloodPressure"]==0].shape)
print("SkinThickness",df[df["SkinThickness"]==0].shape)
print("Insulin",df[df["Insulin"]==0].shape)
print("BMI",df[df["BMI"]==0].shape)

# coverting 0 to mean of their corresponding column.

df.Pregnancies.replace(0,np.mean(df.Pregnancies),inplace=True)
df.Glucose.replace(0,np.mean(df.Glucose),inplace=True)
df.BloodPressure.replace(0,np.mean(df.BloodPressure),inplace=True)
df.SkinThickness.replace(0,np.mean(df.SkinThickness),inplace=True)
df.Insulin.replace(0,np.mean(df.Insulin),inplace=True)
df.BMI.replace(0,np.mean(df.BMI),inplace=True)

# checking for any other presence of data 0.
print("Pregnancies",df[df["Pregnancies"]==0].shape)
print("Glucose",df[df["Glucose"]==0].shape)
print("BloodPressure",df[df["BloodPressure"]==0].shape)
print("SkinThickness",df[df["SkinThickness"]==0].shape)
print("Insulin",df[df["Insulin"]==0].shape)
print("BMI",df[df["BMI"]==0].shape)

#Grouping by the target column and analysing the mean of the dataset. And providing simple conclusion.
#those value for pregnancies, age, BMI,Glucose are playing good role in determining the Output of the dataset.
df.groupby("Outcome").mean()

"""# **Data** **Visualization**"""

#Visualization with pieplot and countplot
fig, axes = plt.subplots(1, 2, figsize=(15, 5))
df["Outcome"].value_counts().plot.pie(ax=axes[0],explode=[0.05,0],autopct="%2.1f%%")
sns.countplot("Outcome",data=df,ax=axes[1])
   
#From the graph the data could be concluded with 65.1% with non diabetic patiants and rest is affected with diabetics.

#Visualization with histplot
sns.histplot(data=df, x="Age")
#The conclusion that can be drawn from here is that most of the patients are from the age group of 20-30

#Visualization with scatterplot
sns.scatterplot(data=df, x="Age", y="DiabetesPedigreeFunction",hue="Outcome")
#As per the data, when a patient gets older he more chance of getting affected by diabetes.

#Visualization with lineplot for some columns against Outcome and sees the relation between them
fig, axes = plt.subplots(3, 2, figsize=(20, 10))
sns.lineplot(data=df, x="Pregnancies", y="Outcome",ax=axes[0][0])
sns.lineplot(data=df, x="Age", y="Outcome",ax=axes[0][1])
sns.lineplot(data=df, x="Glucose", y="Outcome",ax=axes[1][0])
sns.lineplot(data=df, x="BloodPressure", y="Outcome",ax=axes[1][1])
sns.lineplot(data=df, x="SkinThickness", y="Outcome",ax=axes[2][0])
sns.lineplot(data=df, x="BMI", y="Outcome",ax=axes[2][1])

df.head()

#Pairplot Visualization
sns.pairplot(df,hue="Outcome")

#Coorelation
df.corr()

df["Outcome"].value_counts()

# separating the data and labels
x = df.drop('Outcome', 1)
y = df["Outcome"]

x

y

"""# **Data Standardization**"""

scaler=StandardScaler()

scaler.fit(x)

new_x= scaler.transform(x)

new_x

x=new_x
print(x)
print(y)

"""**Testing and Training data**"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,stratify=y, random_state=2)

"""**LogisticRegression**"""

from sklearn.linear_model import LogisticRegression
lr_model = LogisticRegression()
lr_model.fit(x_train, y_train)

#predicting with testing dataset
y_logistic = lr_model.predict(x_test)

y_logistic

#predicting with training dataset
y_logistic1 = lr_model.predict(x_train)

y_logistic1

#classification_report and confusion_matrix
from sklearn import metrics 
from sklearn.metrics import classification_report, confusion_matrix
matrix = confusion_matrix(y_test, y_logistic)
sns.heatmap(matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(y_test, y_logistic))

#accuracy_score for testing dataset
from sklearn.metrics import accuracy_score
lr_accuracy =accuracy_score(y_test,y_logistic)
lr_accuracy

#accuracy_score for training dataset
accuracy_score(y_train,y_logistic1)

"""**DecisionTreeClassifier**"""

from sklearn.tree import DecisionTreeClassifier  
classifier1= DecisionTreeClassifier(criterion='entropy', random_state=2)  
classifier1.fit(x_train, y_train)

#predicting with testing dataset
y_DT= classifier1.predict(x_test)

# accuracy for testing dataset
from sklearn.metrics import accuracy_score
dt_accuracy=accuracy_score(y_DT,y_test)
dt_accuracy

## accuracy for training dataset
y_DT1= classifier1.predict(x_train)  
accuracy_score(y_train,y_DT1)

#Here the data is overfitting

#classification_report and confusion_matrix
from sklearn import metrics 
from sklearn.metrics import classification_report, confusion_matrix
matrix = confusion_matrix(y_test, y_DT)
sns.heatmap(matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(y_test, y_DT))

"""**RandomForestClassifier**"""

from sklearn.ensemble import RandomForestClassifier  
classifier2= RandomForestClassifier(n_estimators= 50, criterion="entropy")  
classifier2.fit(x_train, y_train)

#predicting with testing dataset
y_RF= classifier2.predict(x_test)

# accuracy for testing dataset
from sklearn.metrics import accuracy_score
rt_accuracy=accuracy_score(y_RF,y_test)
rt_accuracy

# accuracy for training dataset
y_RF1= classifier2.predict(x_train)

accuracy_score(y_train,y_RF1)


#Here the data is overfitting

#classification_report and confusion_matrix
from sklearn import metrics 
from sklearn.metrics import classification_report, confusion_matrix
matrix = confusion_matrix(y_test, y_RF)
sns.heatmap(matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(y_test, y_RF))

"""**svm**"""

from sklearn.svm import SVC
classifier3 = SVC(kernel='linear', random_state=0)  
classifier3.fit(x_train, y_train)

#predicting with testing dataset
y_svm= classifier3.predict(x_test)

# accuracy for testing dataset
from sklearn.metrics import accuracy_score
svm_accuracy=accuracy_score(y_svm,y_test)
svm_accuracy

# accuracy for training dataset
y_svm1= classifier3.predict(x_train)

accuracy_score(y_svm1,y_train)

#classification_report and confusion_matrix
from sklearn import metrics 
from sklearn.metrics import classification_report, confusion_matrix
matrix = confusion_matrix(y_test, y_svm)
sns.heatmap(matrix, annot=True, fmt="d")
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
print(classification_report(y_test, y_svm))

print("Logistic Regression accuracy      :",lr_accuracy,"  training :",accuracy_score(y_train,y_logistic1))
print("Decision Tree Classifier accuracy :",dt_accuracy,"  training :",accuracy_score(y_train,y_DT1))
print("Random Forest accuracy            :",rt_accuracy,"  training :",accuracy_score(y_train,y_RF1))
print("SVM accuracy                      :",svm_accuracy,"  training :",accuracy_score(y_svm1,y_train))

"""# **Observation**
From the above predicted model accuracy, we could conclude that SVM classification and Logistic Regression have best accuracy than other models. From these two SVM is performing higher in accuracy than Logistic Regression. So we conclude SVM as the model to be used and proceeding with saving and loading data in a pickle.

# **Save and load the model with Pickle**
"""

import pickle
filename = "my_model.pickle"
pickle.dump(classifier3, open(filename, "wb"))
loaded_model= pickle.load(open(filename, "rb"))

loaded_model.predict(x_test)

"""# **Prediction System**"""

input_data=(3,126,88,41,23,39.3,0.704,27)     #input data
n=np.asarray(input_data)                      #converted into numpy array
reshaped=n.reshape(1,-1)                      #trying to predict for only one instance
std_data=scaler.transform(reshaped)           #Standerdized the data
print(std_data)

predicted=loaded_model.predict(std_data)    #predicting with loaded data
if predicted == 0:
  print("Not Diabetic")
else:
  print("Diabetic")



